{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaaa1004",
   "metadata": {},
   "source": [
    "# 1. PRODUCT WEB CRAWLING\n",
    "1.1. Brand \\\n",
    "1.2. Main image of product \\\n",
    "1.3. Detailed information "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6788e",
   "metadata": {},
   "source": [
    "## 1.1. Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465b2c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727ccc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the ChromeDriver executable\n",
    "path  = r\"C:\\Users\\NT550\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\"\n",
    "s = Service(path)\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.get('https://www.musinsa.com/brands')\n",
    "time.sleep(2)\n",
    "brand = []\n",
    "for page in range(3, 13):\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/section/div[3]/div/div[2]/div/div/a[{0}]'.format(page)).send_keys(Keys.ENTER)\n",
    "    for i in range(1, 101):\n",
    "        name = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/section/div[2]/div/ul/li[{0}]/dl/dt/a'.format(i)).text\n",
    "        name_eng = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/section/div[2]/div/ul/li[{0}]/dl/dd/a'.format(i)).text.split(sep = \"(\")[0].rstrip()\n",
    "        brand.extend([name, name_eng])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26f599cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(brand))\n",
    "total_brand = []\n",
    "total_brand.extend(brand)\n",
    "print(total_brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b48bd",
   "metadata": {},
   "source": [
    "## 1.2. Main image of product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db68b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download image\n",
    "def download_image(url, save_path):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(save_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "file_path = r\"C:\\Users\\z1one\\OneDrive\\바탕 화면\\Portfolio_desktop\\3_Utilization-of-Image-Reviews-from-Online-Shopping-Mall\\DATA\\ProductNum.xlsx\"\n",
    "category_list = pd.read_excel('ProductNum.xlsx',index_col=False)\n",
    "\n",
    "# get main image of goods\n",
    "def get_main_img(goods_id):\n",
    "    # Count the number of photos\n",
    "    k = 0\n",
    "    # Get the product URL\n",
    "    url = f\"https://www.musinsa.com/app/goods/{goods_id}\"\n",
    "    # Use BeautifulSoup\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    try:\n",
    "        # Count the number of main photos\n",
    "        count = len(soup.find_all(\"ul\", {\"class\": \"product_thumb\"})[0].find_all(\"img\"))\n",
    "\n",
    "        # Create a product folder\n",
    "        os.mkdir(str(goods_id))\n",
    "\n",
    "        # Get images\n",
    "        for i in range(count):\n",
    "            # Name for each photo (ProductID_PhotoCount)\n",
    "            temp = str(goods_id) + \"_\" + str(k)\n",
    "\n",
    "            # Get the thumbnail image URL\n",
    "            url1 = soup.find_all(\"ul\", {\"class\": \"product_thumb\"})[0].find_all(\"img\")[i].get(\"src\")\n",
    "\n",
    "            # Replace thumbnail image _60 with _500 using regular expression\n",
    "            url2 = re.sub(r'(?<=\\d{14}_)\\d+(?=\\.jpg)', '500', url1)\n",
    "            file_url = \"https:\" + url2\n",
    "\n",
    "            # Photo save path\n",
    "            save_path = f\"{goods_id}/{temp}(jpg).jpg\"\n",
    "\n",
    "            # Photo download function\n",
    "            download_image(file_url, save_path)\n",
    "\n",
    "            # Count\n",
    "            k += 1\n",
    "            success_list = [goods_id, 1]\n",
    "    except:\n",
    "        fail_list = [goods_id, 0]\n",
    "        return fail_list\n",
    "    return success_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d9bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service(path)\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # Run in headless mode (without a visible browser window)\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# List of goods IDs (example)\n",
    "goods_ids = [1848166, 3105468, 3324814, 3134702]\n",
    "\n",
    "# List to store completed IDs\n",
    "done_list = []\n",
    "\n",
    "# Run the get_main_img function for each goods_id\n",
    "for goods_id in goods_ids:\n",
    "    done_id = get_main_img(goods_id)  # Assuming you have defined and implemented the get_main_img function\n",
    "    done_list.append(done_id)\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "id_list = pd.DataFrame(done_list, columns=[\"goods_id\", \"result\"])\n",
    "\n",
    "# Set the path for saving the DataFrame to a CSV file\n",
    "id_list.to_csv('img_list.csv')\n",
    "\n",
    "# Read a CSV file named \"review/review_4000.csv\" into a DataFrame\n",
    "review = pd.read_csv(\"review/review_4000.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d44bd",
   "metadata": {},
   "source": [
    "## 1.3. Detailed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11233aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_info(id):\n",
    "    url = \"https://www.musinsa.com/app/goods/\"+ str(id) + \"/0\"\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\", \"Accept-Language\": \"ko-kr,ko\"}\n",
    "    res = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    \n",
    "    item_div = soup.find_all('div', class_= \"right_contents section_product_summary\")\n",
    "    \n",
    "    # product_info\n",
    "\t\t\n",
    "    for con in item_div:\n",
    "        if len(con.find('a', href = \"https://www.musinsa.com/categories/item/001\")) !=0:\n",
    "            cat1 = con.find('a', href = \"https://www.musinsa.com/categories/item/001\").text\n",
    "\n",
    "            item['cat1'] = cat1\n",
    "        if len(con.find('a', href = \"https://www.musinsa.com/categories/item/001002\")) !=0:\n",
    "            cat2 = con.find('a', href = \"https://www.musinsa.com/categories/item/001002\").text.split('/')[1]\n",
    "            item['cat2'] = cat2\n",
    "        print(cat1)\n",
    "        print(cat2)\n",
    "        \n",
    "    # product_titles\n",
    "        if len(con.find('span', class_=\"product_title\")) != 0:\n",
    "            product_title = con.find('span', class_=\"product_title\").text.strip()\n",
    "            item['product_title'] = product_title\n",
    "        print(product_title)\n",
    "    \n",
    "    # brands, item num, season, gender\n",
    "        if len(con.find(\"div\", \"explan_product product_info_section\").find('a'))!=0:\n",
    "            # info = con.find(\"div\", \"explan_product product_info_section\").text.strip('').split('\\n')\n",
    "            # info =list(filter(None, info))\n",
    "            brand = con.find(\"div\", \"explan_product product_info_section\").find('a').text\n",
    "            item['brand'] = brand\n",
    "        if len(con.find(\"div\", \"explan_product product_info_section\").find('p', class_= \"product_article_contents\").find('strong'))!=0:\n",
    "            item_num = con.find(\"div\", \"explan_product product_info_section\").find('p', class_= \"product_article_contents\").find('strong').text.split('/')[1].strip(' ')\n",
    "            item['item_num'] = item_num\n",
    "        print(brand)\n",
    "        print(item_num)\n",
    "        \n",
    "    # season, gender\n",
    "\n",
    "        if len(con.find(\"div\", \"explan_product product_info_section\").find_all('p', class_= \"product_article_contents\")[1].find('strong'))!=0:\n",
    "            season = con.find(\"div\", \"explan_product product_info_section\").find_all('p', class_= \"product_article_contents\")[1].find('strong').text.split()\n",
    "            season = ' '.join(season)\n",
    "            item['season'] = season\n",
    "        if len(con.find(\"div\", \"explan_product product_info_section\").find_all('p', class_= \"product_article_contents\")[1].find('span', class_= \"txt_gender\"))!=0:\n",
    "            gender = con.find(\"div\", \"explan_product product_info_section\").find_all('p', class_= \"product_article_contents\")[1].find('span', class_= \"txt_gender\").text.strip()\n",
    "            item['gender'] = gender\n",
    "        print(season)\n",
    "        print(gender)\n",
    "        \n",
    "    # likes, rates, tags\n",
    "        if len(con.find(\"div\", \"explan_product product_info_section\").find('span', class_=\"prd-score__rating\")) != 0:\n",
    "            rate= con.find(\"div\", \"explan_product product_info_section\").find('span', class_=\"prd-score__rating\").text\n",
    "\n",
    "            item['rate'] = con.find(\"div\", \"explan_product product_info_section\").find('span', class_=\"prd-score__rating\").text\n",
    "        if len(con.find(\"div\", \"explan_product product_info_section\").find('li', class_= \"product_section_like\").find('span', class_=\"prd_like_cnt\")) != 0:\n",
    "            likes = con.find(\"div\", \"explan_product product_info_section\").find('li', class_= \"product_section_like\").find('span', class_=\"prd_like_cnt\").text\n",
    "            item['likes'] = likes\n",
    "        if len(con.find(\"div\", \"explan_product product_info_section\").find('li', class_='article-tag-list list').find('p', class_= \"product_article_contents\")) != 0:\n",
    "            tags = con.find(\"div\", \"explan_product product_info_section\").find('li', class_='article-tag-list list').find('p', class_= \"product_article_contents\").text.split('\\n')\n",
    "            tags = ''.join(tags)\n",
    "            item['tags'] = tags\n",
    "        print(rate)\n",
    "        print(likes)\n",
    "        print(tags)\n",
    "        \n",
    "    # color\n",
    "        # colors = []\n",
    "        # cols = con.find('ul', class_='prd-group').find_all('p', class_='n-tooltip tooltip-musinsa tool-bottom')\n",
    "\t\t# \t\tif len(col) !=0:        \n",
    "\t\t# \t\t\tfor col in cols:\n",
    "        #    c = col.text.strip(' \\n')\n",
    "        #    colors.append(c)\n",
    "\n",
    "        # print(colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
